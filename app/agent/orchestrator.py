"""Orchestrator: manages parallel agent execution for multiple sources."""
import asyncio
import json
import logging
import re
import uuid
from collections import defaultdict
from datetime import datetime, date, timedelta

from sqlalchemy import select, delete

from app.database.connection import async_session
from app.models.source import MonitorSource
from app.models.task import CrawlTask, TaskStatus, TriggerType
from app.models.result import CrawlResult
from app.models.report import Report
from app.agent.runtime import run_agent, AgentResult
from app.agent.prompts import build_section_prompt, DEFAULT_CRAWL_RULES
from app.agent.tools.browser import browse_page, close_browser
from app.llm.client import simple_completion
from app.llm.schemas import CRAWLER_TOOLS
from app.notification.engine import dispatch_report
from app.config import AGENT_MAX_CONCURRENCY, LLM_MAX_CONCURRENCY

logger = logging.getLogger(__name__)

# Per-source lock to prevent concurrent runs on the same source
_running_sources: set[int] = set()

# task_id -> asyncio.Event, set() means cancellation requested
_cancel_flags: dict[int, asyncio.Event] = {}


def request_cancel(task_id: int):
    """Mark a task for cancellation (called from the API layer)."""
    if task_id in _cancel_flags:
        _cancel_flags[task_id].set()


def release_source(source_id: int):
    """Remove a source from the running set (called from the API layer on cancel)."""
    _running_sources.discard(source_id)


def is_cancel_requested(task_id: int) -> bool:
    ev = _cancel_flags.get(task_id)
    return ev is not None and ev.is_set()


def is_running() -> bool:
    """Check if ANY source is currently running."""
    return bool(_running_sources)


def get_running_sources() -> set[int]:
    """Return the set of source IDs currently being crawled."""
    return set(_running_sources)


async def run_batch(
    source_ids: list[int] | None = None,
    triggered_by: str = TriggerType.manual.value,
) -> str:
    """Run a crawl batch for the given sources (or all active sources).

    Returns the batch_id.
    """
    batch_id = uuid.uuid4().hex[:12]
    logger.info("Starting batch %s (triggered_by=%s)", batch_id, triggered_by)

    runnable: list[MonitorSource] = []

    try:
        # Fetch sources
        async with async_session() as session:
            query = select(MonitorSource).where(MonitorSource.is_active == True)
            if source_ids:
                query = query.where(MonitorSource.id.in_(source_ids))
            result = await session.execute(query)
            sources = list(result.scalars().all())

        if not sources:
            logger.warning("No active sources found for batch %s", batch_id)
            return batch_id

        # Filter out sources that are already running
        already_running = []
        for src in sources:
            if src.id in _running_sources:
                already_running.append(src.name)
            else:
                runnable.append(src)

        if already_running:
            logger.info("Skipping already-running sources: %s", already_running)

        if not runnable:
            logger.warning("All requested sources are already running")
            return batch_id

        # Mark sources as running
        for src in runnable:
            _running_sources.add(src.id)

        # Create task records
        tasks_map: dict[int, int] = {}  # source_id -> task_id
        async with async_session() as session:
            for src in runnable:
                task = CrawlTask(
                    batch_id=batch_id,
                    source_id=src.id,
                    source_name=src.name,
                    status=TaskStatus.pending.value,
                    triggered_by=triggered_by,
                )
                session.add(task)
            await session.commit()

            # Re-fetch to get IDs
            q = await session.execute(
                select(CrawlTask).where(CrawlTask.batch_id == batch_id)
            )
            for t in q.scalars():
                tasks_map[t.source_id] = t.id

        # Run agents in parallel with concurrency limit
        sem = asyncio.Semaphore(AGENT_MAX_CONCURRENCY)

        async def _limited_run(src, tid, bid):
            async with sem:
                try:
                    await _run_single_source(src, tid, bid)
                finally:
                    _running_sources.discard(src.id)

        agent_tasks = [_limited_run(src, tasks_map[src.id], batch_id) for src in runnable]
        await asyncio.gather(*agent_tasks, return_exceptions=True)

        # Generate and dispatch report
        await _generate_report(batch_id)

    except Exception as e:
        logger.error("Batch %s failed: %s", batch_id, e)
        # Release all locks on error
        for src in runnable:
            _running_sources.discard(src.id)
    finally:
        # Only close browser when no other sources are still running
        # (another concurrent batch may still be using it)
        if not _running_sources:
            await close_browser()

    return batch_id


async def _get_existing_urls(source_id: int) -> list[str]:
    """Fetch all previously crawled URLs for a source (for deduplication)."""
    async with async_session() as session:
        result = await session.execute(
            select(CrawlResult.url).where(CrawlResult.source_id == source_id)
        )
        return [row[0] for row in result.all()]


##############################################################################
# Phase 1a: Homepage â€” extract items (pure code) + identify sections (LLM)
##############################################################################

def _normalize_date(d: str) -> str:
    """Normalize a date string to YYYY-MM-DD with zero-padding.

    Handles formats like '2026-2-3' -> '2026-02-03'.
    """
    parts = d.split('-')
    if len(parts) == 3:
        try:
            return f"{parts[0]}-{parts[1].zfill(2)}-{parts[2].zfill(2)}"
        except Exception:
            pass
    return d


def _extract_homepage_items(
    page_text: str,
    date_start: str,
    date_end: str,
) -> list[dict]:
    """Extract directly-harvestable items from browse_page output (no LLM).

    Looks for the "--- å¯ç›´æ¥é‡‡é›†çš„æ¡ç›®" marker, parses the JSON array,
    filters by date range, and deduplicates by URL.

    Returns: [{"title", "url", "published_date", ...}, ...]
    """
    items_marker = "--- å¯ç›´æ¥é‡‡é›†çš„æ¡ç›®"
    if items_marker not in page_text:
        return []

    items_text = page_text[page_text.index(items_marker):]
    # Find JSON array in the text
    match = re.search(r"\[.*\]", items_text, re.DOTALL)
    if not match:
        return []

    try:
        raw_items = json.loads(match.group(0))
    except json.JSONDecodeError:
        return []

    if not isinstance(raw_items, list):
        return []

    # Filter by date range and deduplicate
    seen_urls = set()
    filtered = []
    for item in raw_items:
        if not isinstance(item, dict):
            continue
        url = item.get("url", "")
        if not url or not item.get("title"):
            continue
        # Normalize http/https
        norm_url = url.replace("http://", "https://")
        if norm_url in seen_urls:
            continue
        seen_urls.add(norm_url)

        pub_date = item.get("published_date", "")
        if pub_date:
            pub_date = _normalize_date(pub_date)
            item["published_date"] = pub_date  # write back normalized
            if pub_date < date_start or pub_date > date_end:
                continue
        # Items without date are kept (may be relevant)
        filtered.append(item)

    return filtered


async def _filter_homepage_items(
    items: list[dict],
    crawl_rules: str,
    on_progress=None,
) -> list[dict]:
    """Use LLM to filter homepage items by crawl_rules, removing low-value content.

    Returns a subset of items that pass the quality filter.
    Falls back to returning all items on failure.
    """
    if len(items) <= 3:
        return items

    lines = []
    for i, item in enumerate(items):
        title = item.get("title", "")
        url = item.get("url", "")[:80]
        date = item.get("published_date", "")
        lines.append(f"[{i}] {date} | {title} | {url}")

    items_text = "\n".join(lines)

    system = "ä½ æ˜¯æ”¿ç­–ä¿¡æ¯ç­›é€‰ä¸“å®¶ï¼ŒæœåŠ¡äºå’¨è¯¢å…¬å¸è¡Œä¸šé¡¾é—®ã€‚è¯·ä¸¥æ ¼æŒ‰ç…§è§„åˆ™ç­›é€‰é«˜ä»·å€¼æ¡ç›®ã€‚"
    user = (
        f"è¯·æ ¹æ®ä»¥ä¸‹é‡‡é›†è§„åˆ™ï¼Œä»é¦–é¡µæå–çš„ {len(items)} æ¡æ¡ç›®ä¸­ç­›é€‰å‡ºå€¼å¾—ä¿ç•™çš„é«˜ä»·å€¼å†…å®¹ã€‚\n\n"
        f"## é‡‡é›†è§„åˆ™\n{crawl_rules}\n\n"
        f"## æ¡ç›®åˆ—è¡¨\n{items_text}\n\n"
        f"ç­›é€‰è¦æ±‚ï¼š\n"
        f"- æ’é™¤åœ°æ–¹ç›‘ç®¡å±€/ç›‘ç®¡åŠçš„æ—¥å¸¸å·¥ä½œåŠ¨æ€\n"
        f"- ä¿ç•™å›½å®¶å±‚é¢æ”¿ç­–ã€é«˜å±‚é¢†å¯¼æ´»åŠ¨ã€å…¨å›½æ€§æ–°é—»æ•°æ®\n"
        f"- ä¸ç¡®å®šçš„æ¡ç›®åº”ä¿ç•™\n"
        f"- è¿”å›ä¿ç•™çš„ç¼–å·JSONæ•°ç»„ï¼Œå¦‚ [0, 3, 5]\n"
        f"- ç›´æ¥è¾“å‡ºJSONï¼Œä¸åŠ å…¶ä»–å†…å®¹"
    )

    try:
        raw = await simple_completion(user, system=system, temperature=0.1, max_tokens=512)
        raw = raw.strip()
        if raw.startswith("```"):
            raw = re.sub(r"^```\w*\n?", "", raw)
            raw = re.sub(r"\n?```$", "", raw)
            raw = raw.strip()
        match = re.search(r"\[.*\]", raw, re.DOTALL)
        if match:
            indices = json.loads(match.group(0))
            if isinstance(indices, list):
                valid = [i for i in indices if isinstance(i, int) and 0 <= i < len(items)]
                if valid:
                    if on_progress:
                        await on_progress(
                            f"Phase 1a: è´¨é‡ç­›é€‰ {len(items)} â†’ {len(valid)} æ¡"
                        )
                    logger.info("Homepage filter: %d -> %d items", len(items), len(valid))
                    return [items[i] for i in valid]
    except (json.JSONDecodeError, Exception) as e:
        logger.warning("Homepage item filtering failed, keeping all: %s", e)

    return items


async def _identify_sections(
    page_text: str,
    source: MonitorSource,
    on_progress=None,
) -> list[dict]:
    """Use LLM to identify section list-page URLs from the homepage.

    Injects source.crawl_rules into the prompt.
    Returns: [{"name": "æ ç›®å", "url": "åˆ—è¡¨é¡µURL"}, ...]
    Falls back to [{"name": source.name, "url": source.url}] on failure.
    """
    fallback = [{"name": source.name, "url": source.url}]
    crawl_rules = source.crawl_rules or DEFAULT_CRAWL_RULES

    # Extract just the link list section
    link_section = ""
    link_marker = "--- é¡µé¢é“¾æ¥åˆ—è¡¨ ---"
    if link_marker in page_text:
        link_section = page_text[page_text.index(link_marker):]
        items_marker = "--- å¯ç›´æ¥é‡‡é›†çš„æ¡ç›®"
        if items_marker in link_section:
            link_section = link_section[:link_section.index(items_marker)]
    if not link_section:
        link_section = page_text[:8000]

    system = "ä½ æ˜¯ç½‘é¡µç»“æ„åˆ†æä¸“å®¶ã€‚è¯·ä»é“¾æ¥åˆ—è¡¨ä¸­è¯†åˆ«å‡ºå€¼å¾—æ·±å…¥é‡‡é›†çš„æ ç›®åˆ—è¡¨é¡µURLã€‚"
    user = (
        f"ä»¥ä¸‹æ˜¯ {source.name}ï¼ˆ{source.url}ï¼‰é¦–é¡µçš„é“¾æ¥åˆ—è¡¨ã€‚\n"
        f"è¯·ä»ä¸­æ‰¾å‡ºå€¼å¾—æ·±å…¥é‡‡é›†çš„æ ç›®åˆ—è¡¨é¡µé“¾æ¥ã€‚\n\n"
        f"## æ ç›®ç­›é€‰è§„åˆ™ï¼ˆè¯·ä¸¥æ ¼éµå®ˆï¼‰\n{crawl_rules}\n\n"
        f"è¦æ±‚ï¼š\n"
        f"- è¿”å›JSONæ•°ç»„ï¼š[{{\"name\": \"æ ç›®å\", \"url\": \"åˆ—è¡¨é¡µå®Œæ•´URL\"}}]\n"
        f"- åªè¿”å›èƒ½è¿›å…¥æ–‡ç« åˆ—è¡¨çš„æ ç›®é¡µé“¾æ¥ï¼ˆå¦‚ /zcfg/ã€/tzgg/ã€/gzdt/ ç­‰æ ç›®å…¥å£ï¼‰ï¼Œä¸è¦å…·ä½“æ–‡ç« è¯¦æƒ…é“¾æ¥\n"
        f"- æ ç›®å…¥å£URLé€šå¸¸è¾ƒçŸ­ã€ä¸å«æ—¥æœŸï¼Œæ–‡ç« URLé€šå¸¸è¾ƒé•¿ã€å«æ—¥æœŸè·¯å¾„\n"
        f"- å¦‚æœæ‰¾åˆ°å¤šä¸ªåŒ¹é…æ ç›®ï¼Œéƒ½åˆ—å‡ºæ¥\n"
        f"- ç›´æ¥è¾“å‡ºJSONï¼Œä¸åŠ å…¶ä»–å†…å®¹\n\n"
        f"é“¾æ¥åˆ—è¡¨ï¼š\n{link_section}"
    )

    try:
        raw = await simple_completion(user, system=system, temperature=0.1, max_tokens=2048)
        raw = raw.strip()
        if raw.startswith("```"):
            raw = re.sub(r"^```\w*\n?", "", raw)
            raw = re.sub(r"\n?```$", "", raw)
            raw = raw.strip()
        match = re.search(r"\[.*\]", raw, re.DOTALL)
        if match:
            raw = match.group(0)
        sections = json.loads(raw)
        if isinstance(sections, list) and sections:
            valid = [s for s in sections if isinstance(s, dict) and s.get("url")]
            if valid:
                if on_progress:
                    await on_progress(f"Phase 1a: å‘ç° {len(valid)} ä¸ªæ ç›®")
                logger.info("[%s] Homepage navigation found %d sections", source.name, len(valid))
                return valid
    except (json.JSONDecodeError, Exception) as e:
        logger.warning("[%s] Homepage navigation LLM parse failed: %s", source.name, e)

    if on_progress:
        await on_progress("Phase 1a: æ ç›®æå–å¤±è´¥ï¼Œé™çº§ä¸ºç›´æ¥ä½¿ç”¨æºURL")
    return fallback


##############################################################################
# Phase 1b: Section-level crawling â€” independent sub-agents per section
##############################################################################

async def _crawl_all_sections(
    source: MonitorSource,
    sections: list[dict],
    existing_urls: list[str],
    cancel_event: asyncio.Event | None,
    on_progress=None,
    crawl_rules: str = "",
) -> list[dict]:
    """Run a crawler sub-agent for each section (serial), return merged items.

    Each sub-agent gets a clean context window with enable_pruning=True.
    Later sections receive URLs from earlier sections for cross-section dedup.
    """
    time_range_days = source.time_range_days or 7
    max_items = source.max_items or 30
    today = datetime.now()
    start_date = today - timedelta(days=time_range_days)
    date_range = f"{start_date.strftime('%Y-%m-%d')} è‡³ {today.strftime('%Y-%m-%d')}"

    all_items: list[dict] = []
    collected_urls = set(existing_urls)

    for idx, section in enumerate(sections):
        if cancel_event and cancel_event.is_set():
            break

        section_name = section.get("name", f"æ ç›®{idx + 1}")
        section_url = section.get("url", "")
        if not section_url:
            continue

        if on_progress:
            await on_progress(f"Phase 1b: é‡‡é›†æ ç›® ({idx + 1}/{len(sections)}): {section_name}")

        # Build prompt with cross-section dedup URLs
        section_prompt = build_section_prompt(
            section_name=section_name,
            section_url=section_url,
            date_range=date_range,
            max_items=max_items - len(all_items),  # remaining quota
            existing_urls=list(collected_urls) if collected_urls else None,
            crawl_rules=crawl_rules,
        )

        user_msg = f"è¯·å¼€å§‹é‡‡é›†æ ç›®ã€Œ{section_name}ã€çš„åˆ—è¡¨é¡µï¼š{section_url}"

        try:
            agent_result = await run_agent(
                source,
                existing_urls=list(collected_urls),
                on_progress=on_progress,
                cancel_event=cancel_event,
                system_prompt=section_prompt,
                user_message=user_msg,
                tools=CRAWLER_TOOLS,
                max_turns=15,
                enable_pruning=True,
            )

            # Collect items and update URL set for next section
            for item in agent_result.items:
                url = item.get("url", "")
                if url and url not in collected_urls:
                    collected_urls.add(url)
                    all_items.append(item)

            logger.info("[%s] Section '%s': %d items", source.name, section_name, len(agent_result.items))

        except Exception as e:
            logger.error("[%s] Section '%s' agent failed: %s", source.name, section_name, e)
            if on_progress:
                await on_progress(f"æ ç›® {section_name} é‡‡é›†å¤±è´¥: {e}")
            continue

        # Stop if we've reached the max
        if len(all_items) >= max_items:
            break

    return all_items


##############################################################################
# Phase 2: Summary agent â€” concurrent simple_completion per item
##############################################################################

async def _summarize_items(
    items: list[dict],
    cancel_event: asyncio.Event | None,
    on_progress=None,
):
    """Generate summaries for items that don't have one.

    Each item gets an independent simple_completion call with clean context.
    Runs with bounded concurrency (LLM_MAX_CONCURRENCY).
    """
    needs_summary = [i for i in items if not i.get("summary")]
    if not needs_summary:
        return

    if on_progress:
        await on_progress(f"Phase 2: ä¸º {len(needs_summary)} æ¡å†…å®¹ç”Ÿæˆæ‘˜è¦")
    logger.info("Phase 2: generating summaries for %d items", len(needs_summary))

    sem = asyncio.Semaphore(LLM_MAX_CONCURRENCY)
    summary_system = (
        "ä½ æ˜¯æ”¿ç­–æƒ…æŠ¥åˆ†æå¸ˆï¼ŒæœåŠ¡äºå’¨è¯¢å…¬å¸çš„è¡Œä¸šé¡¾é—®å›¢é˜Ÿã€‚\n"
        "è¯·æ ¹æ®æä¾›çš„æ–‡ç« æ­£æ–‡æ’°å†™ä¸€æ®µç®€æ˜æ‘˜è¦ï¼Œå¸®åŠ©é¡¾é—®å¿«é€Ÿäº†è§£æ–‡ç« æ ¸å¿ƒå†…å®¹ã€‚"
    )

    async def _process_one(item, idx):
        if cancel_event and cancel_event.is_set():
            return
        url = item.get("url", "")
        title = item.get("title", "")
        if not url:
            return

        async with sem:
            try:
                if on_progress:
                    await on_progress(f"æ‘˜è¦ ({idx + 1}/{len(needs_summary)}): {title[:50]}")

                page_text = await browse_page(url)
                if not page_text or "é¡µé¢åŠ è½½å¤±è´¥" in page_text:
                    return

                user_prompt = (
                    f"è¯·ä¸ºä»¥ä¸‹æ–‡ç« æ’°å†™æ‘˜è¦ã€‚\n\n"
                    f"è¦æ±‚ï¼š\n"
                    f"- 2-3å¥è¯ï¼Œ100-200å­—\n"
                    f"- æç‚¼æ ¸å¿ƒæ”¿ç­–è¦ç‚¹ã€å…³é”®æ•°æ®æˆ–ä¸»è¦æªæ–½\n"
                    f"- ä¸è¦é‡å¤æ ‡é¢˜å†…å®¹\n"
                    f"- ç›´æ¥è¾“å‡ºæ‘˜è¦ï¼Œä¸åŠ å‰ç¼€\n\n"
                    f"æ ‡é¢˜ï¼š{title}\n\n"
                    f"æ­£æ–‡ï¼š\n{page_text[:6000]}"
                )

                summary = await simple_completion(
                    user_prompt, system=summary_system, temperature=0.2, max_tokens=512
                )
                summary = summary.strip()

                # Validate
                if not summary or summary == title.strip() or len(summary) < 20:
                    # Retry once
                    summary = await simple_completion(
                        user_prompt, system=summary_system, temperature=0.3, max_tokens=512
                    )
                    summary = summary.strip()

                if summary and summary != title.strip() and len(summary) >= 20:
                    item["summary"] = summary
            except Exception as e:
                logger.warning("Summary failed for %s: %s", url, e)

    await asyncio.gather(
        *[_process_one(item, idx) for idx, item in enumerate(needs_summary)],
        return_exceptions=True,
    )

    generated = sum(1 for i in needs_summary if i.get("summary"))
    if on_progress:
        await on_progress(f"Phase 2: å®Œæˆï¼Œ{generated}/{len(needs_summary)} æ¡æ‘˜è¦ç”ŸæˆæˆåŠŸ")
    logger.info("Phase 2 done: %d/%d summaries generated", generated, len(needs_summary))


##############################################################################
# Phase 3: Ranking agent â€” single simple_completion
##############################################################################

async def _rank_items(items: list[dict], on_progress=None) -> list[dict]:
    """Rank items by strategic importance using a single LLM call.

    Falls back to date-descending order on failure.
    """
    if len(items) <= 1:
        return items

    if on_progress:
        await on_progress("Phase 3: æŒ‰æˆ˜ç•¥é‡è¦æ€§æ’åº")
    logger.info("Phase 3: ranking %d items", len(items))

    # Build compact text: [i] [type] date | title â€” summary[:80]
    type_map = {"news": "æ–°é—»", "policy": "æ”¿ç­–", "notice": "é€šçŸ¥", "file": "æ–‡ä»¶"}
    lines = []
    for i, item in enumerate(items):
        type_label = type_map.get(item.get("content_type", ""), "å†…å®¹")
        d = item.get("published_date", "")
        title = item.get("title", "")
        summary_snippet = (item.get("summary") or "")[:80]
        line = f"[{i}] [{type_label}] {d} | {title}"
        if summary_snippet:
            line += f" â€” {summary_snippet}"
        lines.append(line)

    items_text = "\n".join(lines)

    system = "ä½ æ˜¯å’¨è¯¢å…¬å¸é«˜çº§æ”¿ç­–é¡¾é—®ï¼Œè´Ÿè´£ä¸ºä¼ä¸šå®¢æˆ·ç­›é€‰å’Œæ’åºæ”¿ç­–æƒ…æŠ¥ã€‚ä½ éå¸¸å–„äºåŒºåˆ†å›½å®¶çº§å’Œåœ°æ–¹çº§å†…å®¹çš„é‡è¦æ€§å·®å¼‚ã€‚"
    user = (
        f"è¯·å°†ä»¥ä¸‹{len(items)}æ¡æ”¿ç­–/æ–°é—»æ¡ç›®æŒ‰æˆ˜ç•¥é‡è¦æ€§ä»é«˜åˆ°ä½æ’åºã€‚\n\n"
        f"æ’åºåŸåˆ™ï¼ˆä¸¥æ ¼æŒ‰å±‚çº§æ’åºï¼Œé«˜å±‚çº§çš„ä¸€å®šæ’åœ¨ä½å±‚çº§å‰é¢ï¼‰ï¼š\n\n"
        f"ç¬¬ä¸€å±‚ï¼ˆæœ€é‡è¦ï¼‰ï¼š\n"
        f"- å›½å®¶å±‚é¢é‡å¤§æ”¿ç­–ï¼šå›½åŠ¡é™¢ã€éƒ¨å§”å‘å¸ƒçš„æ³•å¾‹æ³•è§„ã€è§„åˆ’çº²è¦ã€æŒ‡å¯¼æ„è§ã€æ”¹é©æ–¹æ¡ˆ\n"
        f"- é«˜çº§é¢†å¯¼äººï¼ˆå›½å®¶çº§ã€éƒ¨çº§ï¼‰è®²è¯ã€æ‰¹ç¤ºã€ç½²åæ–‡ç« \n"
        f"- é«˜çº§é¢†å¯¼äººäº‹ä»»å…ï¼ˆéƒ¨çº§åŠä»¥ä¸Šï¼‰\n\n"
        f"ç¬¬äºŒå±‚ï¼š\n"
        f"- å…¨å›½æ€§é‡è¦ä¼šè®®ï¼ˆå›½åŠ¡é™¢å¸¸åŠ¡ä¼šè®®ã€éƒ¨å§”å·¥ä½œä¼šè®®ã€å…¨å›½æ€§è¡Œä¸šä¼šè®®ï¼‰\n"
        f"- å…¨å›½æ€§é‡å¤§æ–°é—»ï¼ˆå…¨å›½æ•°æ®å‘å¸ƒã€é‡å¤§é¡¹ç›®ã€è¡Œä¸šé‡Œç¨‹ç¢‘ï¼‰\n"
        f"- å›½å®¶çº§è¡Œä¸šæ ‡å‡†ã€è§„èŒƒå‘å¸ƒ\n\n"
        f"ç¬¬ä¸‰å±‚ï¼š\n"
        f"- éƒ¨å§”é€šçŸ¥ã€å…¬å‘Š\n"
        f"- è¡Œä¸šç»Ÿè®¡æ•°æ®ã€å‘å±•æŠ¥å‘Š\n"
        f"- æ”¿ç­–è§£è¯»ã€ç­”è®°è€…é—®\n\n"
        f"ç¬¬å››å±‚ï¼š\n"
        f"- åœ°æ–¹æ€§æ”¿ç­–æ–‡ä»¶ã€çœçº§é€šçŸ¥\n"
        f"- åœ°æ–¹é¡¹ç›®æ ¸å‡†ã€åœ°æ–¹ä¼šè®®\n\n"
        f"ç¬¬äº”å±‚ï¼ˆæœ€ä¸é‡è¦ï¼‰ï¼š\n"
        f"- åœ°æ–¹ç›‘ç®¡å±€æ—¥å¸¸å·¥ä½œåŠ¨æ€\n"
        f"- æ¥è®¿æ¥å¾…ã€è°ƒç ”è§†å¯Ÿï¼ˆéé«˜çº§é¢†å¯¼ï¼‰\n"
        f"- ä¸€èˆ¬æ€§å·¥ä½œç®€æŠ¥\n\n"
        f"å…³é”®åˆ¤æ–­æ–¹æ³•ï¼šæ ‡é¢˜ä¸­å«æœ‰\"å›½åŠ¡é™¢\"\"å›½å®¶\"\"å…¨å›½\"\"éƒ¨\"ç­‰å…³é”®è¯çš„é€šå¸¸æ˜¯ç¬¬ä¸€ã€äºŒå±‚ï¼›å«æœ‰çœä»½åã€\"XXå±€\"\"XXåŠ\"ç­‰åœ°æ–¹æœºæ„åçš„é€šå¸¸æ˜¯ç¬¬å››ã€äº”å±‚ã€‚\n"
        f"åŒä¸€å±‚çº§å†…ï¼Œæ—¥æœŸè¾ƒæ–°çš„ä¼˜å…ˆã€‚\n\n"
        f"è¯·åªè¿”å›æ’åºåçš„ç¼–å·JSONæ•°ç»„ï¼Œå¦‚ [3, 0, 7, 1, 5]\n"
        f"ä¸è¦è¾“å‡ºä»»ä½•å…¶ä»–å†…å®¹ã€‚\n\n"
        f"æ¡ç›®åˆ—è¡¨ï¼š\n{items_text}"
    )

    try:
        raw = await simple_completion(user, system=system, temperature=0.1, max_tokens=1024)
        raw = raw.strip()
        if raw.startswith("```"):
            raw = re.sub(r"^```\w*\n?", "", raw)
            raw = re.sub(r"\n?```$", "", raw)
            raw = raw.strip()

        sorted_indices = json.loads(raw)

        if isinstance(sorted_indices, list):
            # Validate: integers in range
            valid = [i for i in sorted_indices if isinstance(i, int) and 0 <= i < len(items)]
            # Append any missing indices
            seen = set(valid)
            for i in range(len(items)):
                if i not in seen:
                    valid.append(i)

            ranked = [items[i] for i in valid]
            if on_progress:
                await on_progress("Phase 3: æ’åºå®Œæˆ")
            logger.info("Phase 3: ranking succeeded")
            return ranked

    except (json.JSONDecodeError, Exception) as e:
        logger.warning("Phase 3 ranking failed, falling back to date sort: %s", e)

    # Fallback: sort by date descending
    if on_progress:
        await on_progress("Phase 3: æ’åºå¤±è´¥ï¼Œé™çº§ä¸ºæŒ‰æ—¥æœŸæ’åº")

    def sort_key(item):
        d = item.get("published_date", "")
        return d if d else "0000-00-00"
    items.sort(key=sort_key, reverse=True)
    return items


##############################################################################
# Main pipeline: _run_single_source
##############################################################################

async def _run_single_source(source: MonitorSource, task_id: int, batch_id: str):
    """Run the 4-phase pipeline for a single source and persist results."""
    cancel_event = asyncio.Event()
    _cancel_flags[task_id] = cancel_event

    # Mark task as running
    async with async_session() as session:
        task = await session.get(CrawlTask, task_id)
        task.status = TaskStatus.running.value
        task.started_at = datetime.utcnow()
        await session.commit()

    try:
        async def _on_progress(msg: str):
            try:
                async with async_session() as sess:
                    t = await sess.get(CrawlTask, task_id)
                    timestamp = datetime.utcnow().strftime("%H:%M:%S")
                    t.progress_log = (t.progress_log or "") + f"[{timestamp}] {msg}\n"
                    await sess.commit()
            except Exception:
                pass

        existing_urls = await _get_existing_urls(source.id)
        max_items = source.max_items or 30
        crawl_rules = source.crawl_rules or DEFAULT_CRAWL_RULES

        time_range_days = source.time_range_days or 7
        today = datetime.now()
        start_date = today - timedelta(days=time_range_days)
        date_start = start_date.strftime('%Y-%m-%d')
        date_end = today.strftime('%Y-%m-%d')

        # â”€â”€ Phase 1a: Browse homepage, extract items + identify sections â”€â”€
        if is_cancel_requested(task_id):
            logger.info("[%s] Task %d cancelled", source.name, task_id)
            return

        await _on_progress("Phase 1a: æµè§ˆé¦–é¡µï¼Œæå–æ¡ç›®å’Œæ ç›®é“¾æ¥")

        try:
            homepage_text = await browse_page(source.url)
        except Exception as e:
            logger.warning("[%s] Failed to browse homepage: %s", source.name, e)
            homepage_text = ""

        if not homepage_text or "é¡µé¢åŠ è½½å¤±è´¥" in homepage_text:
            homepage_text = ""

        # Step 1: Extract directly-harvestable items (pure code, no LLM)
        homepage_items = _extract_homepage_items(homepage_text, date_start, date_end) if homepage_text else []

        # Step 2: Identify sections via LLM (with crawl_rules injection)
        sections = await _identify_sections(homepage_text, source, on_progress=_on_progress) if homepage_text else [{"name": source.name, "url": source.url}]

        await _on_progress(f"Phase 1a: é¦–é¡µæå– {len(homepage_items)} æ¡æ¡ç›®ï¼Œ{len(sections)} ä¸ªæ ç›®")

        # Step 3: LLM quality filter â€” apply crawl_rules to homepage items
        if homepage_items:
            homepage_items = await _filter_homepage_items(
                homepage_items, crawl_rules, on_progress=_on_progress,
            )

        await _on_progress(f"Phase 1a: ç­›é€‰åä¿ç•™ {len(homepage_items)} æ¡é¦–é¡µæ¡ç›®")

        if is_cancel_requested(task_id):
            logger.info("[%s] Task %d cancelled", source.name, task_id)
            return

        # â”€â”€ Phase 1b: Selective section crawling (Plan B) â”€â”€
        remaining = max_items - len(homepage_items)
        if remaining <= 0:
            sections_to_crawl = []
            await _on_progress("Phase 1b: é¦–é¡µæ¡ç›®å·²è¶³å¤Ÿï¼Œè·³è¿‡æ ç›®è¡¥å……é‡‡é›†")
        else:
            sections_to_crawl = sections[:3]  # At most 3 supplementary sections
            await _on_progress(f"Phase 1b: è¡¥å……é‡‡é›† {len(sections_to_crawl)} ä¸ªæ ç›®")

        section_items = []
        if sections_to_crawl:
            # Pass homepage item URLs to avoid duplicates
            homepage_urls = [item.get("url", "") for item in homepage_items]
            combined_existing = existing_urls + homepage_urls

            section_items = await _crawl_all_sections(
                source, sections_to_crawl, combined_existing, cancel_event,
                on_progress=_on_progress, crawl_rules=crawl_rules,
            )

        if is_cancel_requested(task_id):
            logger.info("[%s] Task %d cancelled", source.name, task_id)
            return

        # Merge and deduplicate
        all_items = homepage_items + section_items
        existing_url_set = set(u.replace("http://", "https://") for u in existing_urls)
        seen_urls = set()
        deduped_items = []
        for item in all_items:
            url = item.get("url", "")
            norm_url = url.replace("http://", "https://")
            if norm_url in existing_url_set or norm_url in seen_urls:
                continue
            seen_urls.add(norm_url)
            deduped_items.append(item)

        # Trim to max_items
        if len(deduped_items) > max_items:
            def sort_key(item):
                d = item.get("published_date", "")
                return d if d else "0000-00-00"
            deduped_items.sort(key=sort_key, reverse=True)
            deduped_items = deduped_items[:max_items]

        # â”€â”€ Phase 2: Summary generation â”€â”€
        if is_cancel_requested(task_id):
            logger.info("[%s] Task %d cancelled", source.name, task_id)
            return

        await _summarize_items(deduped_items, cancel_event, on_progress=_on_progress)

        # â”€â”€ Phase 3: Strategic ranking â”€â”€
        if is_cancel_requested(task_id):
            logger.info("[%s] Task %d cancelled", source.name, task_id)
            return

        deduped_items = await _rank_items(deduped_items, on_progress=_on_progress)

        # â”€â”€ Persist results â”€â”€
        if is_cancel_requested(task_id):
            logger.info("[%s] Task %d cancelled", source.name, task_id)
            return

        async with async_session() as session:
            for item in deduped_items:
                pub_date = None
                if item.get("published_date"):
                    try:
                        pub_date = date.fromisoformat(item["published_date"])
                    except ValueError:
                        pass

                cr = CrawlResult(
                    task_id=task_id,
                    source_id=source.id,
                    title=item["title"],
                    url=item["url"],
                    content_type=item.get("content_type", "news"),
                    summary=item.get("summary", ""),
                    has_attachment=item.get("has_attachment", False),
                    attachment_name=item.get("attachment_name", ""),
                    attachment_type=item.get("attachment_type", ""),
                    attachment_path=item.get("attachment_path", ""),
                    attachment_summary=item.get("attachment_summary", ""),
                    published_date=pub_date,
                )
                session.add(cr)
            await session.commit()

        # Mark task as completed
        async with async_session() as session:
            task = await session.get(CrawlTask, task_id)
            task.status = TaskStatus.completed.value
            task.completed_at = datetime.utcnow()
            task.items_found = len(deduped_items)
            await session.commit()

        logger.info("[%s] Pipeline done: %d items persisted", source.name, len(deduped_items))

    except Exception as e:
        logger.error("[%s] Pipeline crashed: %s", source.name, e)
        async with async_session() as session:
            task = await session.get(CrawlTask, task_id)
            task.status = TaskStatus.failed.value
            task.completed_at = datetime.utcnow()
            task.error_log = str(e)
            await session.commit()
    finally:
        _cancel_flags.pop(task_id, None)


async def _generate_overview(by_source: dict[str, list[CrawlResult]]) -> str:
    """Use LLM to generate a structured overview of all results."""
    # Build a condensed summary of all items for the LLM
    summary_parts = []
    for src_name, items in by_source.items():
        summary_parts.append(f"ã€{src_name}ã€‘å…±{len(items)}æ¡:")
        for item in items[:20]:  # Limit to avoid token overflow
            line = f"- [{item.content_type}] {item.title}"
            if item.summary:
                line += f": {item.summary[:150]}"
            summary_parts.append(line)

    all_summaries = "\n".join(summary_parts)

    system = (
        "ä½ æ˜¯å’¨è¯¢å…¬å¸é«˜çº§è¡Œä¸šé¡¾é—®ï¼Œæ“…é•¿æ’°å†™ç»“æ„æ¸…æ™°ã€é‡ç‚¹çªå‡ºçš„æ”¿ç­–æƒ…æŠ¥ç®€æŠ¥ã€‚"
        "ä½ çš„è¯»è€…æ˜¯ä¼ä¸šé«˜ç®¡å’Œè¡Œä¸šåˆ†æå¸ˆï¼Œä»–ä»¬éœ€è¦å¿«é€ŸæŠŠæ¡æ”¿ç­–é£å‘å’Œè¡Œä¸šåŠ¨æ€ã€‚"
    )

    prompt = f"""è¯·æ ¹æ®ä»¥ä¸‹é‡‡é›†æ¡ç›®ï¼Œæ’°å†™ä¸€ä»½ç»“æ„åŒ–çš„æ”¿ç­–æƒ…æŠ¥æ¦‚è¿°ï¼ˆ300-600å­—ï¼‰ã€‚

æŒ‰ä»¥ä¸‹æ¨¡æ¿è¾“å‡ºï¼ˆ## æ ‡é¢˜ç‹¬å ä¸€è¡Œï¼Œæ­£æ–‡å¦èµ·ä¸€è¡Œï¼Œæ®µè½ä¹‹é—´ç©ºä¸€è¡Œï¼‰ï¼š

## æ ¸å¿ƒè¦ç‚¹

1-2å¥è¯ç‚¹æ˜æœ¬æœŸæœ€é‡è¦çš„æ”¿ç­–ä¿¡å·æˆ–è¡Œä¸šå˜åŒ–ã€‚

## é‡å¤§æ”¿ç­–åŠ¨å‘

å¦‚æœ‰å›½å®¶çº§æ”¿ç­–ã€æ³•è§„ã€è§„åˆ’ï¼Œé˜è¿°å…¶è¦ç‚¹å’Œå½±å“ï¼ˆ2-3å¥è¯ï¼‰ã€‚

## è¡Œä¸šæ•°æ®ä¸è¶‹åŠ¿

å¦‚æœ‰ç»Ÿè®¡æ•°æ®å‘å¸ƒã€è¡Œä¸šé‡Œç¨‹ç¢‘ï¼Œæç‚¼å…³é”®æ•°å­—ï¼ˆ2-3å¥è¯ï¼‰ã€‚

## ç›‘ç®¡ä¸æ‰§è¡ŒåŠ¨æ€

å¦‚æœ‰ç›‘ç®¡è¡ŒåŠ¨ã€åœ°æ–¹æ‰§è¡Œã€æ ‡å‡†å‘å¸ƒï¼Œç®€è¦å½’çº³ï¼ˆ2-3å¥è¯ï¼‰ã€‚

ä¸¥æ ¼æ ¼å¼è¦æ±‚ï¼š
- æ¯ä¸ªéƒ¨åˆ†ä»¥ ## æ ‡é¢˜å¼€å¤´ï¼Œæ ‡é¢˜ç‹¬å ä¸€è¡Œï¼Œæ ‡é¢˜åç©ºä¸€è¡Œå†å†™æ­£æ–‡
- æ­£æ–‡ä¸­ç”¨ **ç²—ä½“** å¼ºè°ƒå…³é”®ä¿¡æ¯ï¼ˆå¦‚æ”¿ç­–åç§°ã€æ•°æ®ï¼‰
- å¦‚æœæŸä¸ªéƒ¨åˆ†æ²¡æœ‰å¯¹åº”å†…å®¹ï¼Œç›´æ¥çœç•¥è¯¥éƒ¨åˆ†
- ä¸è¦ä½¿ç”¨ç¼–å·åˆ—è¡¨ï¼ˆ1. 2. 3.ï¼‰ï¼Œç”¨è‡ªç„¶æ®µè½å™è¿°
- ç”¨å…·ä½“æ•°æ®å’Œäº‹å®è¯´è¯ï¼Œé¿å…ç©ºæ³›è¯„ä»·
- ç›´æ¥è¾“å‡ºï¼Œä¸åŠ "æ¦‚è¿°""ä»¥ä¸‹æ˜¯"ç­‰å‰ç¼€

é‡‡é›†æ¡ç›®ï¼š
{all_summaries}"""

    try:
        overview = await simple_completion(prompt, system=system, temperature=0.3, max_tokens=1500)
        return overview.strip()
    except Exception as e:
        logger.error("Failed to generate overview: %s", e)
        return ""


def _overview_to_html(text: str) -> str:
    """Convert markdown-style overview text to clean, naturally readable HTML.

    Handles:
    - ## headings â†’ styled <h3>
    - **bold** â†’ <strong>
    - Numbered sections (1. **title** ...) â†’ heading + paragraph
    - Bullet lists (- item / * item) â†’ <ul><li>
    - Paragraphs separated by blank lines
    - Strips all remaining markdown artifacts
    """
    if not text:
        return ""

    # Normalize line endings
    text = text.replace("\r\n", "\n").strip()

    # Convert **bold** to <strong>
    text = re.sub(r"\*\*(.+?)\*\*", r"<strong>\1</strong>", text)

    lines = text.split("\n")
    html_parts: list[str] = []
    current_body: list[str] = []
    current_list: list[str] = []

    p_style = "margin:6px 0 14px 0;line-height:1.8;color:#374151;"
    heading_style = (
        "margin:16px 0 4px 0;font-size:15px;font-weight:600;"
        "color:#1e40af;border-bottom:1px solid #e5e7eb;padding-bottom:4px;"
    )
    li_style = "margin:2px 0;line-height:1.7;color:#374151;"
    ul_style = "margin:6px 0 14px 0;padding-left:20px;color:#374151;"

    def _flush_body():
        if current_body:
            body = " ".join(current_body).strip()
            if body:
                html_parts.append(f'<p style="{p_style}">{body}</p>')
            current_body.clear()

    def _flush_list():
        if current_list:
            items_html = "".join(
                f'<li style="{li_style}">{item}</li>' for item in current_list
            )
            html_parts.append(f'<ul style="{ul_style}">{items_html}</ul>')
            current_list.clear()

    for line in lines:
        stripped = line.strip()
        if not stripped:
            _flush_body()
            _flush_list()
            continue

        # Match ## heading
        m_hash = re.match(r"^#{1,3}\s+(.+)$", stripped)
        if m_hash:
            _flush_body()
            _flush_list()
            html_parts.append(f'<h3 style="{heading_style}">{m_hash.group(1)}</h3>')
            continue

        # Match bullet list: - text or * text
        m_bullet = re.match(r"^[-*]\s+(.+)$", stripped)
        if m_bullet:
            _flush_body()
            current_list.append(m_bullet.group(1))
            continue

        # If we were building a list and hit non-list content, flush it
        _flush_list()

        # Match inline numbered heading + body: "1. <strong>æ ¸å¿ƒè¦ç‚¹</strong> some content"
        m_inline = re.match(
            r"^(\d+)\.\s*<strong>(.+?)</strong>\s*(.+)$", stripped
        )
        if m_inline:
            _flush_body()
            html_parts.append(f'<h3 style="{heading_style}">{m_inline.group(2)}</h3>')
            current_body.append(m_inline.group(3))
            continue

        # Match short numbered heading: "1. æ ¸å¿ƒè¦ç‚¹" or "1. <strong>æ ¸å¿ƒè¦ç‚¹</strong>"
        m_num = re.match(
            r"^(\d+)\.\s*(?:<strong>)?(.+?)(?:</strong>)?\s*$", stripped
        )
        if m_num and len(stripped) < 40:
            _flush_body()
            html_parts.append(f'<h3 style="{heading_style}">{m_num.group(2)}</h3>')
            continue

        # Regular body text
        current_body.append(stripped)

    _flush_body()
    _flush_list()

    if not html_parts:
        # Fallback: wrap as paragraph
        return f'<p style="{p_style}">{text}</p>'

    return "\n".join(html_parts)


async def _generate_report(batch_id: str):
    """Generate a report from the batch results and dispatch notifications."""
    async with async_session() as session:
        # Fetch all results for this batch
        tasks_q = await session.execute(
            select(CrawlTask).where(CrawlTask.batch_id == batch_id)
        )
        tasks = list(tasks_q.scalars().all())

        results_q = await session.execute(
            select(CrawlResult).where(
                CrawlResult.task_id.in_([t.id for t in tasks])
            ).order_by(CrawlResult.source_id, CrawlResult.published_date.desc())
        )
        results = list(results_q.scalars().all())

    if not results:
        logger.info("Batch %s: no results to report", batch_id)
        return

    # Group by source
    by_source: dict[str, list[CrawlResult]] = defaultdict(list)
    for r in results:
        # Find source name from tasks
        src_name = next((t.source_name for t in tasks if t.source_id == r.source_id), f"æº{r.source_id}")
        by_source[src_name].append(r)

    # Generate aggregated overview via LLM
    overview = await _generate_overview(by_source)

    # Build title: {æºåç§°}æ›´æ–°æ±‡æ€»æŠ¥å‘ŠYYYY-MM-DD
    now = datetime.now()
    source_names = "ã€".join(by_source.keys())
    title = f"{source_names}æ›´æ–°æ±‡æ€»æŠ¥å‘Š{now.strftime('%Y-%m-%d')}"

    # Build HTML
    html_parts = [f"<h1>{title}</h1>"]

    # Overview section
    if overview:
        overview_html = _overview_to_html(overview)
        html_parts.append('<div style="margin:20px 0;padding:20px;background:#f0f7ff;border-radius:8px;border-left:4px solid #1a56db;">')
        html_parts.append('<h2 style="margin:0 0 12px 0;color:#1a56db;font-size:18px;">æ•´ä½“æ¦‚è¿°</h2>')
        html_parts.append(overview_html)
        html_parts.append('</div>')
        html_parts.append('<hr style="margin:24px 0;border-color:#e5e7eb;">')

    # Build plain text
    text_parts = [title, "=" * 40]

    if overview:
        text_parts.append("\nã€æ•´ä½“æ¦‚è¿°ã€‘")
        text_parts.append(overview)
        text_parts.append("\n" + "-" * 40)

    # Per-source sections
    for src_name, items in by_source.items():
        html_parts.append(f'<h2 style="border-left:4px solid #1a56db;padding-left:12px;">{src_name} Â· {len(items)} æ¡æ›´æ–°</h2>')
        text_parts.append(f"\n== {src_name} ({len(items)}æ¡æ›´æ–°) ==\n")

        for i, item in enumerate(items, 1):
            # HTML
            type_label = {"news": "æ–°é—»", "policy": "æ”¿ç­–", "notice": "é€šçŸ¥", "file": "æ–‡ä»¶"}.get(item.content_type, "å†…å®¹")
            html_parts.append(f'<div style="margin:16px 0;padding:12px;border:1px solid #e5e7eb;border-radius:8px;">')
            html_parts.append(f'<p style="margin:0;"><strong>[{type_label}] {item.title}</strong></p>')
            if item.published_date:
                html_parts.append(f'<p style="color:#6b7280;font-size:14px;">å‘å¸ƒæ—¥æœŸï¼š{item.published_date}</p>')
            # Only show summary if it's meaningful (not empty, not same as title)
            has_real_summary = item.summary and item.summary.strip() != item.title.strip()
            if has_real_summary:
                html_parts.append(f'<p style="margin:8px 0;">{item.summary}</p>')
            if item.has_attachment and item.attachment_name:
                html_parts.append(f'<p>ğŸ“ é™„ä»¶: {item.attachment_name}</p>')
                if item.attachment_summary:
                    html_parts.append(f'<p style="color:#4b5563;font-size:14px;">é™„ä»¶æ‘˜è¦: {item.attachment_summary}</p>')
            html_parts.append(f'<p><a href="{item.url}" style="color:#1a56db;">ğŸ“– æŸ¥çœ‹åŸæ–‡</a></p>')
            html_parts.append('</div>')

            # Plain text
            text_parts.append(f"{i}. [{type_label}] {item.title}")
            if item.published_date:
                text_parts.append(f"   æ—¥æœŸ: {item.published_date}")
            if has_real_summary:
                text_parts.append(f"   > {item.summary[:200]}")
            if item.has_attachment:
                text_parts.append(f"   ğŸ“ é™„ä»¶: {item.attachment_name}")
            text_parts.append(f"   é“¾æ¥: {item.url}")
            text_parts.append("")

    html_parts.append('<hr style="margin:24px 0;">')
    html_parts.append('<p style="color:#9ca3af;font-size:12px;">æ­¤é‚®ä»¶ç”±æ”¿ç­–æƒ…æŠ¥åŠ©æ‰‹è‡ªåŠ¨ç”Ÿæˆï¼ˆAIæ‘˜è¦ä»…ä¾›å‚è€ƒï¼‰</p>')

    content_html = "\n".join(html_parts)
    content_text = "\n".join(text_parts)

    # Save report
    async with async_session() as session:
        report = Report(
            batch_id=batch_id,
            title=title,
            content_html=content_html,
            content_text=content_text,
            overview=overview,
        )
        session.add(report)
        await session.commit()
        report_id = report.id

    logger.info("Report generated: %s (id=%d)", title, report_id)

    # Dispatch notifications
    await dispatch_report(batch_id, title, content_html, content_text, results)
